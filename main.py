from layers.load_data import load_CIFAR10
from layers.ResNet import ResNet
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
import matplotlib.pyplot as plt
import torchvision

# device = 'cuda' if torch.cuda.is_available() else 'cpu'
device = torch.device('cuda:0')


def get_lr(optimizer):
    for param_group in optimizer.param_groups:
        return param_group['lr']


def train(model, epochs, train_loader, scheduler, optimizer, criterion):
    model.train()
    x = []
    y = []

    for epoch in range(epochs):
        running_loss = 0.0
        acc = 0
        correct = 0
        total = 0
        lr=0

        # if epoch == 1:
        #     for g in optimizer.param_groups:
        #         g['lr']=0.1

        for i, data in  enumerate(train_loader):
            scheduler.step()
            lr = get_lr(optimizer)
            inputs, labels = data # training_set에서 데이터와 label을 분리
            inputs, labels = inputs.to(device), labels.to(device) # 쿠다로 학습하겠다

            # gradients를 0으로
            optimizer.zero_grad()

            outputs = model(inputs) # model에 input넣기
            loss = criterion(outputs, labels) # output값과 실제값을 비교해 crossentropy적용
            loss.backward() # loss에서 나온값으로 backpropagatoin 적용
            optimizer.step() # 파라미터 업데이트

            # print statics
            running_loss += loss.item()

            _, predicted = outputs.max(1) # output값 중 제일 큰 값이 레이블에 가장 가까운 값
            total += labels.size(0) # 훈련시킨 데이터의 총 개수
            correct += predicted.eq(labels).sum().item() # correct는 맞은것의 개수

        acc = 100 * correct / total

        y.append(100-acc)
        x.append(epoch)

        print('train epoch : {} [{}/{}]| loss: {:.3f} | acc: {:.3f} LR: {:.4f}'.format(
            epoch, i, len(train_loader), running_loss / (i + 1), acc, lr))

    print(x)
    print(y)

    # visualsize(x, y)
    torch.save(model.state_dict(), "model_CIFAR-10_110_scheduler.bin")


def test(model, epochs, test_loader, criterion):
    model.eval()

    for epoch in range(epochs):
        test_loss = 0
        total = 0
        correct = 0
        for i, data in enumerate(test_loader):
            inputs, labels = data
            inputs.to(device), labels.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, labels)

            test_loss += loss.item()
            _, predicted = outputs.max(1)

            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

        acc = 100 * correct / total
        print('test epoch : {} [{}/{}]| loss: {:.3f} | acc: {:.3f}'.format(
            epoch, i, len(test_loader), test_loss / (i + 1), acc))


def main():

    # visualsize(0,0)

    print("load data")
    train_loader, test_loader = load_CIFAR10()

    print("make model")
    resnet = ResNet(block_numbers=5, class_numbers=10)
    print(resnet)
    resnet = resnet.to(device)

    # weight_day, momentum, lr 사용할수있는 경사하강법, resnet.parameters()를 갱신시켜라!
    optimizer = optim.SGD(resnet.parameters(), weight_decay=0.0001, momentum=0.9, lr=0.01)
    # optimizer_init = optim.SGD(resnet.parameters(), weight_decay=0.0001, momentum=0.9, lr=0.01)
    # learning_rate를 10씩 감소시켜줄 scheduler
    scheduler = lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[32000, 48000], gamma=0.1)
    criterion = nn.CrossEntropyLoss()

    print("training start")
    # 164번은 해야 64000범의 ieteration
    train(model=resnet, epochs=164, train_loader=train_loader, scheduler=scheduler, optimizer=optimizer, criterion=criterion)

    """
    # 테스트하는부분
    """
    # print("load saved model")
    # pre_resnet = ResNet(block_numbers=5, class_numbers=10)
    # pre_resnet.load_state_dict(torch.load("model_CIFAR-10_32_scheduler.bin"))
    # for param in pre_resnet.parameters():
    #     param.requires_gard = False
    #
    # print("test start")
    # test(model=pre_resnet, epochs=10, test_loader=test_loader, criterion=criterion)


def visualsize(x, y):

    # 20
    # [62.544, 41.718, 32.134, 27.034000000000006,23.908,
    y_20=[21.751999999999995, 20.046000000000006, 18.909999999999997, 17.950000000000003, 17.191999999999993, 16.575999999999993, 15.884, 15.367999999999995, 14.558000000000007, 14.305999999999997, 13.965999999999994, 13.668000000000006, 13.230000000000004, 13.224000000000004, 12.602000000000004, 12.316000000000003, 12.284000000000006, 12.063999999999993, 11.858000000000004, 11.486000000000004, 11.373999999999995, 11.331999999999994, 10.896, 10.867999999999995, 10.792000000000002, 10.683999999999997, 10.546000000000006, 10.420000000000002, 10.474000000000004, 10.25, 10.135999999999996, 9.962000000000003, 10.048000000000002, 9.932000000000002, 9.846000000000004, 9.872, 9.438000000000002, 9.549999999999997, 9.388000000000005, 9.555999999999997, 9.408000000000001, 9.372, 9.006, 9.156000000000006, 8.811999999999998, 8.903999999999996, 8.906000000000006, 8.858000000000004, 8.804000000000002, 8.994, 8.829999999999998, 8.658000000000001, 8.593999999999994, 8.604, 8.421999999999997, 8.792000000000002, 8.349999999999994, 8.450000000000003, 8.567999999999998, 8.354, 8.552000000000007, 8.373999999999995, 8.25, 8.189999999999998, 8.14, 8.239999999999995, 8.156000000000006, 7.963999999999999, 8.224000000000004, 8.018, 8.081999999999994, 7.900000000000006, 8.180000000000007, 7.908000000000001, 7.890000000000001, 7.930000000000007, 8.0, 4.447999999999993, 3.7660000000000053, 3.4599999999999937, 3.2900000000000063, 2.989999999999995, 2.823999999999998, 2.7099999999999937, 2.632000000000005, 2.5160000000000053, 2.548000000000002, 2.3640000000000043, 2.436000000000007, 2.1880000000000024, 2.078000000000003, 2.078000000000003, 2.087999999999994, 2.010000000000005, 1.9879999999999995, 1.8859999999999957, 1.921999999999997, 1.7459999999999951, 1.695999999999998, 1.7879999999999967, 1.6479999999999961, 1.4920000000000044, 1.5960000000000036, 1.5520000000000067, 1.4099999999999966, 1.4159999999999968, 1.5400000000000063, 1.5259999999999962, 1.3719999999999999, 1.4339999999999975, 1.4479999999999933, 1.313999999999993, 1.3400000000000034, 1.3520000000000039, 1.3119999999999976, 1.3299999999999983, 1.274000000000001, 1.230000000000004, 1.0220000000000056, 0.9519999999999982, 0.9440000000000026, 0.8319999999999936, 0.847999999999999, 0.9099999999999966, 0.8340000000000032, 0.8179999999999978, 0.7459999999999951, 0.8059999999999974, 0.7620000000000005, 0.7819999999999965, 0.7319999999999993, 0.7579999999999956, 0.7480000000000047, 0.7180000000000035, 0.7219999999999942, 0.7620000000000005, 0.7180000000000035, 0.7060000000000031, 0.6599999999999966, 0.6700000000000017, 0.7360000000000042, 0.730000000000004, 0.6620000000000061, 0.7480000000000047, 0.7459999999999951, 0.6800000000000068, 0.715999999999994, 0.6620000000000061, 0.671999999999997, 0.6919999999999931, 0.652000000000001, 0.6219999999999999, 0.7180000000000035, 0.6880000000000024, 0.6839999999999975, 0.6479999999999961, 0.6620000000000061, 0.6080000000000041, 0.6200000000000045]
    # 0, 1, 2, 3,4,
    x=[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163]

    # 32
    # [64.62, 45.6, 34.215999999999994, 27.629999999999995,23.677999999999997,
    y_32= [21.372, 19.715999999999994, 18.736000000000004, 17.317999999999998, 16.727999999999994, 15.884, 15.275999999999996, 14.549999999999997, 14.370000000000005, 13.775999999999996, 13.206000000000003, 12.921999999999997, 12.331999999999994, 12.25, 12.159999999999997, 11.828000000000003, 11.475999999999999, 11.093999999999994, 10.971999999999994, 10.748000000000005, 10.629999999999995, 10.510000000000005, 10.168000000000006, 10.090000000000003, 10.0, 9.701999999999998, 9.540000000000006, 9.543999999999997, 9.304000000000002, 9.376000000000005, 9.170000000000002, 8.982, 8.884, 9.061999999999998, 8.658000000000001, 8.486000000000004, 8.510000000000005, 8.769999999999996, 8.495999999999995, 8.275999999999996, 8.331999999999994, 8.159999999999997, 8.165999999999997, 8.018, 8.153999999999996, 8.194000000000003, 7.941999999999993, 7.778000000000006, 7.837999999999994, 7.936000000000007, 7.494, 7.518000000000001, 7.8940000000000055, 7.867999999999995, 7.4339999999999975, 7.689999999999998, 7.510000000000005, 7.3359999999999985, 7.447999999999993, 7.492000000000004, 7.248000000000005, 7.412000000000006, 7.331999999999994, 7.269999999999996, 7.171999999999997, 7.1299999999999955, 7.158000000000001, 7.063999999999993, 7.224000000000004, 7.102000000000004, 7.072000000000003, 6.974000000000004, 7.1200000000000045, 6.682000000000002, 6.8359999999999985, 7.007999999999996, 6.616, 3.245999999999995, 2.686000000000007, 2.2120000000000033, 2.0660000000000025, 2.0819999999999936, 1.9699999999999989, 1.7219999999999942, 1.6159999999999997, 1.5400000000000063, 1.5300000000000011, 1.4240000000000066, 1.304000000000002, 1.3199999999999932, 1.1700000000000017, 1.1440000000000055, 1.078000000000003, 1.024000000000001, 1.0559999999999974, 1.0520000000000067, 0.9759999999999991, 0.9140000000000015, 0.8919999999999959, 0.9620000000000033, 0.9240000000000066, 0.8419999999999987, 0.784000000000006, 0.7759999999999962, 0.7680000000000007, 0.8319999999999936, 0.6200000000000045, 0.7600000000000051, 0.6219999999999999, 0.6039999999999992, 0.6599999999999966, 0.6599999999999966, 0.6880000000000024, 0.597999999999999, 0.5739999999999981, 0.6299999999999955, 0.5739999999999981, 0.6419999999999959, 0.4539999999999935, 0.41200000000000614, 0.38400000000000034, 0.37000000000000455, 0.382000000000005, 0.347999999999999, 0.3259999999999934, 0.3400000000000034, 0.3199999999999932, 0.32200000000000273, 0.27200000000000557, 0.3160000000000025, 0.2920000000000016, 0.30400000000000205, 0.29999999999999716, 0.29000000000000625, 0.2740000000000009, 0.25799999999999557, 0.25799999999999557, 0.2819999999999965, 0.284000000000006, 0.2780000000000058, 0.26600000000000534, 0.25199999999999534, 0.27599999999999625, 0.2960000000000065, 0.27200000000000557, 0.2540000000000049, 0.2459999999999951, 0.23799999999999955, 0.284000000000006, 0.23399999999999466, 0.2459999999999951, 0.23799999999999955, 0.20600000000000307, 0.29000000000000625, 0.20600000000000307, 0.22400000000000375, 0.2259999999999991, 0.18200000000000216, 0.2079999999999984]

    # 44
    # [66.06, 45.78, 35.132000000000005, 28.522000000000006,24.810000000000002,
    y_44 =[21.936000000000007, 20.144000000000005, 18.396, 17.317999999999998, 16.620000000000005, 15.668000000000006, 15.037999999999997, 14.317999999999998, 13.89, 13.427999999999997, 12.784000000000006, 12.400000000000006, 12.028000000000006, 11.840000000000003, 11.608000000000004, 11.244, 10.843999999999994, 10.719999999999999, 10.302000000000007, 10.293999999999997, 9.914000000000001, 10.058000000000007, 9.674000000000007, 9.546000000000006, 9.480000000000004, 9.242000000000004, 8.989999999999995, 8.725999999999999, 8.757999999999996, 8.641999999999996, 8.75, 8.498000000000005, 8.396, 8.366, 8.0, 8.078000000000003, 7.867999999999995, 7.947999999999993, 7.823999999999998, 7.733999999999995, 7.670000000000002, 7.647999999999996, 7.558000000000007, 7.584000000000003, 7.510000000000005, 7.349999999999994, 7.323999999999998, 7.081999999999994, 7.248000000000005, 7.341999999999999, 6.938000000000002, 7.134, 6.939999999999998, 6.9680000000000035, 6.859999999999999, 7.084000000000003, 6.6440000000000055, 6.813999999999993, 6.853999999999999, 6.786000000000001, 6.707999999999998, 6.408000000000001, 6.686000000000007, 6.656000000000006, 6.635999999999996, 6.611999999999995, 6.534000000000006, 6.774000000000001, 6.403999999999996, 6.225999999999999, 6.284000000000006, 6.5460000000000065, 6.195999999999998, 6.302000000000007, 6.352000000000004, 6.2819999999999965, 5.944000000000003, 2.8739999999999952, 2.146000000000001, 1.8880000000000052, 1.6740000000000066, 1.5019999999999953, 1.3499999999999943, 1.2199999999999989, 1.0660000000000025, 1.024000000000001, 0.9500000000000028, 0.965999999999994, 0.8940000000000055, 0.8100000000000023, 0.7920000000000016, 0.7199999999999989, 0.6620000000000061, 0.6680000000000064, 0.6239999999999952, 0.6099999999999994, 0.5799999999999983, 0.519999999999996, 0.4759999999999991, 0.4959999999999951, 0.5720000000000027, 0.46999999999999886, 0.3960000000000008, 0.49800000000000466, 0.4240000000000066, 0.42600000000000193, 0.39799999999999613, 0.4000000000000057, 0.4159999999999968, 0.38800000000000523, 0.3199999999999932, 0.36599999999999966, 0.3340000000000032, 0.3640000000000043, 0.33199999999999363, 0.34600000000000364, 0.31799999999999784, 0.3359999999999985, 0.2259999999999991, 0.22799999999999443, 0.19199999999999307, 0.1599999999999966, 0.15800000000000125, 0.19199999999999307, 0.1700000000000017, 0.16800000000000637, 0.132000000000005, 0.1599999999999966, 0.15399999999999636, 0.16400000000000148, 0.14400000000000546, 0.19400000000000261, 0.1460000000000008, 0.14400000000000546, 0.1280000000000001, 0.10999999999999943, 0.1560000000000059, 0.13599999999999568, 0.14000000000000057, 0.13400000000000034, 0.12000000000000455, 0.132000000000005, 0.0799999999999983, 0.13800000000000523, 0.13400000000000034, 0.14000000000000057, 0.1419999999999959, 0.09399999999999409, 0.1560000000000059, 0.12600000000000477, 0.11199999999999477, 0.12000000000000455, 0.11400000000000432, 0.12600000000000477, 0.10999999999999943, 0.10599999999999454, 0.11199999999999477, 0.10800000000000409, 0.10800000000000409]
    # 44 26=89.942, 51=92.676, 77=93.804, 102=99.480, 128=99.808, 154=99.858, 163=99.892
    # 44 prediction = 93.060

    # 56 26=90.208, 51=92.914, 77=93.848, 102=99.644, 128=99.904, 154=99.910, 163=99.932
    # 56
    # [71.586, 57.952, 45.49, 36.78,31.438000000000002,
    y_56= [26.483999999999995, 23.337999999999994, 21.150000000000006, 19.732, 17.947999999999993, 16.968000000000004, 16.081999999999994, 15.266000000000005, 14.61, 13.786000000000001, 13.438000000000002, 12.691999999999993, 12.268, 12.147999999999996, 11.584000000000003, 11.075999999999993, 10.982, 10.780000000000001, 10.689999999999998, 10.25, 10.096000000000004, 9.792000000000002, 9.646, 9.394000000000005, 9.215999999999994, 9.141999999999996, 9.052000000000007, 8.924000000000007, 8.516000000000005, 8.349999999999994, 8.328000000000003, 8.219999999999999, 8.028000000000006, 8.164000000000001, 8.012, 7.896000000000001, 7.742000000000004, 7.4879999999999995, 7.486000000000004, 7.555999999999997, 7.424000000000007, 7.286000000000001, 7.323999999999998, 7.319999999999993, 7.090000000000003, 7.134, 7.0859999999999985, 7.022000000000006, 6.843999999999994, 6.906000000000006, 6.8700000000000045, 6.677999999999997, 6.7379999999999995, 6.733999999999995, 6.811999999999998, 6.6440000000000055, 6.457999999999998, 6.554000000000002, 6.689999999999998, 6.441999999999993, 6.3160000000000025, 6.396000000000001, 6.305999999999997, 6.257999999999996, 6.058000000000007, 6.373999999999995, 5.843999999999994, 6.245999999999995, 6.215999999999994, 5.805999999999997, 6.224000000000004, 5.8160000000000025, 6.152000000000001, 5.828000000000003, 6.174000000000007, 5.989999999999995, 5.599999999999994, 2.469999999999999, 1.5999999999999943, 1.4939999999999998, 1.293999999999997, 1.132000000000005, 0.9680000000000035, 0.9500000000000028, 0.8379999999999939, 0.7759999999999962, 0.6560000000000059, 0.5999999999999943, 0.6260000000000048, 0.6200000000000045, 0.5240000000000009, 0.5, 0.5079999999999956, 0.4000000000000057, 0.4479999999999933, 0.45999999999999375, 0.382000000000005, 0.35599999999999454, 0.36199999999999477, 0.32200000000000273, 0.3340000000000032, 0.2639999999999958, 0.25, 0.25799999999999557, 0.2740000000000009, 0.2360000000000042, 0.26600000000000534, 0.24200000000000443, 0.23399999999999466, 0.1740000000000066, 0.20600000000000307, 0.21399999999999864, 0.2459999999999951, 0.24399999999999977, 0.20999999999999375, 0.2219999999999942, 0.20399999999999352, 0.20999999999999375, 0.15800000000000125, 0.13599999999999568, 0.14400000000000546, 0.13400000000000034, 0.12000000000000455, 0.09600000000000364, 0.12399999999999523, 0.10999999999999943, 0.09399999999999409, 0.0799999999999983, 0.11599999999999966, 0.12600000000000477, 0.10599999999999454, 0.10800000000000409, 0.0799999999999983, 0.08599999999999852, 0.08599999999999852, 0.10599999999999454, 0.09600000000000364, 0.10800000000000409, 0.117999999999995, 0.09600000000000364, 0.07800000000000296, 0.0799999999999983, 0.08799999999999386, 0.11199999999999477, 0.11599999999999966, 0.06799999999999784, 0.07800000000000296, 0.060000000000002274, 0.09600000000000364, 0.09000000000000341, 0.08799999999999386, 0.060000000000002274, 0.09199999999999875, 0.0759999999999934, 0.07800000000000296, 0.07800000000000296, 0.09199999999999875, 0.07399999999999807, 0.06799999999999784]
    # 56 prediction = 93.150

    # 110 26=90.112, 51=93.190, 77=94.406, 102=99.782, 128=99.916, 154=99.964, 163=99.942
    # 110
    # [67.262, 60.064, 41.648, 33.396, 28.334000000000003,
    y_110 =[24.471999999999994, 22.310000000000002, 20.367999999999995, 18.944000000000003, 17.590000000000003, 16.653999999999996, 15.939999999999998, 14.754000000000005, 14.480000000000004, 13.858000000000004, 13.116, 12.591999999999999, 12.311999999999998, 12.031999999999996, 11.768, 11.219999999999999, 11.028000000000006, 10.864000000000004, 10.596000000000004, 10.287999999999997, 9.995999999999995, 9.888000000000005, 9.445999999999998, 9.463999999999999, 9.311999999999998, 8.882000000000005, 8.825999999999993, 8.653999999999996, 8.555999999999997, 8.384, 8.296000000000006, 7.885999999999996, 8.081999999999994, 8.043999999999997, 7.8700000000000045, 7.706000000000003, 7.329999999999998, 7.534000000000006, 7.408000000000001, 7.445999999999998, 6.945999999999998, 7.010000000000005, 6.9680000000000035, 6.9539999999999935, 6.689999999999998, 6.841999999999999, 6.810000000000002, 6.653999999999996, 6.670000000000002, 6.828000000000003, 6.349999999999994, 6.590000000000003, 6.349999999999994, 6.519999999999996, 6.397999999999996, 6.426000000000002, 6.1440000000000055, 6.256, 6.3160000000000025, 6.024000000000001, 5.9339999999999975, 5.841999999999999, 5.9339999999999975, 5.912000000000006, 5.980000000000004, 6.0, 5.849999999999994, 5.819999999999993, 5.861999999999995, 5.769999999999996, 5.7180000000000035, 5.760000000000005, 5.593999999999994, 5.6059999999999945, 5.632000000000005, 5.638000000000005, 5.495999999999995, 2.2519999999999953, 1.441999999999993, 1.1680000000000064, 1.0420000000000016, 0.8700000000000045, 0.7980000000000018, 0.6560000000000059, 0.5859999999999985, 0.5439999999999969, 0.5120000000000005, 0.41200000000000614, 0.4000000000000057, 0.36599999999999966, 0.36199999999999477, 0.3299999999999983, 0.30800000000000693, 0.3199999999999932, 0.2960000000000065, 0.2780000000000058, 0.2459999999999951, 0.21800000000000352, 0.23000000000000398, 0.2459999999999951, 0.2259999999999991, 0.24399999999999977, 0.1880000000000024, 0.17799999999999727, 0.16599999999999682, 0.1700000000000017, 0.19599999999999795, 0.15200000000000102, 0.14400000000000546, 0.15000000000000568, 0.1460000000000008, 0.132000000000005, 0.117999999999995, 0.12199999999999989, 0.1839999999999975, 0.12600000000000477, 0.16200000000000614, 0.1560000000000059, 0.09000000000000341, 0.08599999999999852, 0.08199999999999363, 0.07800000000000296, 0.0660000000000025, 0.08400000000000318, 0.09399999999999409, 0.04399999999999693, 0.06199999999999761, 0.04800000000000182, 0.0759999999999934, 0.058000000000006935, 0.06799999999999784, 0.060000000000002274, 0.07399999999999807, 0.04999999999999716, 0.05200000000000671, 0.055999999999997385, 0.04200000000000159, 0.054000000000002046, 0.07399999999999807, 0.054000000000002046, 0.06399999999999295, 0.06399999999999295, 0.0660000000000025, 0.04800000000000182, 0.036000000000001364, 0.04000000000000625, 0.04200000000000159, 0.05200000000000671, 0.04399999999999693, 0.036000000000001364, 0.04399999999999693, 0.02599999999999625, 0.058000000000006935, 0.060000000000002274, 0.055999999999997385, 0.054000000000002046, 0.05200000000000671, 0.05200000000000671, 0.058000000000006935]
    # 110 prediction = 93.250

    plt.plot(x, y_20, label='resnet-20')
    plt.plot(x, y_32, label='resnet-32')
    plt.plot(x, y_44, label='resnet-44')
    plt.plot(x, y_56, label='resnet-56')
    plt.plot(x, y_110, label='resnet-110')

    plt.title('train error')
    plt.xlabel('iter(10000)')
    plt.ylabel('error')
    plt.legend()

    plt.xticks([26, 51, 77, 102, 128, 154], ['1', '2', '3', '4', '5', '6'])
    plt.yticks([20, 10, 5], ['20', '10', '5'])

    plt.show()
    plt.savefig('cifar_44.png')


def testmain():
    densenet121 = torchvision.models.densenet121()
    print(densenet121)


if __name__ == '__main__':
    main()
